# NSM-10 Cross-Domain Comparison

**Date**: 2025-10-20
**Linear Issue**: [NSM-10](https://linear.app/imajn/issue/NSM-10)
**Status**: âœ… All three dataset domains implemented

---

## Executive Summary

Successfully implemented three parallel dataset exploration branches for NSM Phase 1:

1. **Planning Domain** (`dataset-planning` branch)
2. **Knowledge Graph Domain** (`dataset-knowledge-graph` branch)
3. **Causal Reasoning Domain** (`dataset-causal` branch)

All three implementations:
- Extend `BaseSemanticTripleDataset` from NSM-18
- Include comprehensive test suites (21-27 tests each)
- Provide domain-specific evaluation metrics
- Include interactive example scripts
- Are ready for comparative evaluation

**Total Implementation**: ~5,350 lines of production code + tests across three branches

---

## Implementation Overview

| Aspect | Planning | Knowledge Graph | Causal |
|--------|----------|----------------|--------|
| **Branch** | `dataset-planning` | `dataset-knowledge-graph` | `dataset-causal` |
| **Worktree** | `../nsm-planning` | `../nsm-kg` | `../nsm-causal` |
| **Dataset Class** | `PlanningTripleDataset` | `KnowledgeGraphTripleDataset` | `CausalTripleDataset` |
| **Code (lines)** | 1,741 | 2,000 | 1,856 |
| **Tests** | 25 tests | 21 tests | 27 tests |
| **Test Coverage** | Not specified | 98% | 94% |
| **Commits** | 5 commits | 5 commits | 5 commits |

---

## Dataset Characteristics

### Scale and Structure

| Property | Planning | Knowledge Graph | Causal |
|----------|----------|----------------|--------|
| **Total Scenarios** | 1,000 problems | 20,000 triples | 2,000 scenarios |
| **Entities** | ~315 | ~5,000 | ~1,400 |
| **Predicates** | 16 types | 50+ types | 20 types |
| **Total Triples** | ~515 (20 problems) | 20,000 | ~14,000 |
| **L1/L2 Distribution** | 69%/31% | 87%/13% | 50%/50% |

### Confidence Distribution

| Domain | Confidence Range | Average | Variance | Interpretation |
|--------|-----------------|---------|----------|----------------|
| **Planning** | 0.7 - 1.0 | 0.903 | Low | High certainty in procedural knowledge |
| **Knowledge Graph** | 0.5 - 1.0 | 0.77 | High | Partial observability, uncertain facts |
| **Causal** | Variable | Not specified | Very High | Strength of causal effects |

### Hierarchical Structure

| Domain | Level 1 (Concrete) | Level 2 (Abstract) | Hierarchy Naturalness |
|--------|-------------------|-------------------|---------------------|
| **Planning** | Actions, Environment | Goals, Capabilities | â­â­â­â­â­ Very Natural |
| **Knowledge Graph** | Facts, Instances | Types, Categories | â­â­â­ Moderate |
| **Causal** | Observations, Events | Mechanisms, Responses | â­â­â­â­ Natural |

---

## Domain-Specific Features

### Planning Domain

**Triple Examples**:
```python
# Level 1
("robot", "move_to", "location_A", 0.9, level=1)
("robot", "pick_up", "block_1", 0.85, level=1)
("location_A", "contains", "block_1", 0.95, level=1)

# Level 2
("robot", "achieve", "stack_ABC", 0.7, level=2)
("stack_ABC", "requires", "pick_up", 0.8, level=2)
```

**Domain Properties**:
- âœ… Hierarchical goal decomposition
- âœ… Temporal ordering with prerequisites
- âœ… Clear ground truth (valid/invalid sequences)
- âœ… Testable goal achievement

**Evaluation Metrics**:
- Goal achievement rate
- Invalid sequence detection (Acc, Prec, Rec, F1)
- Temporal ordering accuracy
- Capability coverage
- Decomposition accuracy

**Expected Strengths**:
- Strong hierarchical structure (WHY/WHAT alignment)
- Clear procedural reasoning
- Interpretable failures

**Expected Challenges**:
- Low predicate diversity (16 types)
- May be too structured (overly deterministic)

---

### Knowledge Graph Domain

**Triple Examples**:
```python
# Level 1
("Albert_Einstein", "born_in", "Ulm", 0.99, level=1)
("Albert_Einstein", "won", "Nobel_Prize_1921", 0.99, level=1)
("Ulm", "located_in", "Germany", 1.0, level=1)

# Level 2
("Albert_Einstein", "instance_of", "Physicist", 0.95, level=2)
("Physicist", "typically_wins", "Scientific_Awards", 0.7, level=2)
```

**Domain Properties**:
- âœ… Entity-centric reasoning
- âœ… Rich predicate vocabulary (50+ types)
- âœ… Type hierarchies (instance_of, subclass_of)
- âœ… Partial observability

**Evaluation Metrics**:
- Link prediction (Hits@K, MRR, Mean Rank)
- Analogical reasoning (A:B :: C:?)
- Type consistency (Prec, Rec, F1)
- Multi-hop reasoning
- Calibration (ECE, MCE)

**Expected Strengths**:
- Rich relational diversity (tests R-GCN)
- Multi-hop reasoning paths
- Entity-centric interpretability

**Expected Challenges**:
- Weaker hierarchical structure
- Random generation may create noise
- May need deeper hierarchies

---

### Causal Domain

**Triple Examples**:
```python
# Level 1
("aspirin", "taken_by", "patient_42", 0.9, level=1)
("patient_42", "has_symptom", "headache", 0.95, level=1)
("patient_42", "symptom_reduced", "headache", 0.8, level=1)

# Level 2
("aspirin", "causes", "pain_reduction", 0.85, level=2)
("pain_reduction", "treats", "headache", 0.9, level=2)
```

**Domain Properties**:
- âœ… Causal relationships (treatment â†’ effect)
- âœ… Counterfactual reasoning support
- âœ… Confounder modeling
- âœ… Confidence as effect size

**Evaluation Metrics**:
- Counterfactual accuracy
- Confounder detection (F1)
- Effect size estimation (MAE)
- Causal vs correlation distinction
- Intervention prediction accuracy
- Calibration (ECE)

**Expected Strengths**:
- Tests causal reasoning (frontier AI capability)
- Counterfactual queries
- Confidence has epistemic meaning
- WHY/WHAT maps to cause/effect

**Expected Challenges**:
- Confounders add complexity
- May require specialized inference
- Balance treatment assignment randomness

---

## Comparative Analysis

### Predicate Diversity

```
Planning:        16 types  [â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Low
Causal:          20 types  [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Medium
Knowledge Graph: 50+ types [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘] High
```

**Insight**: KG will stress-test R-GCN basis decomposition most effectively

---

### Temporal Structure

```
Planning:        Strong    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] Explicit prerequisites
Causal:          Medium    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘] Treatment â†’ outcome
Knowledge Graph: Weak      [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Mostly atemporal facts
```

**Insight**: Planning tests temporal reasoning, KG tests static knowledge

---

### Hierarchy Naturalness

```
Planning:        Very High [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] Goals â†’ actions natural
Causal:          High      [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] Mechanisms â†’ observations
Knowledge Graph: Moderate  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] Types â†’ instances
```

**Insight**: Planning aligns best with BDI-HTN-HRL framework

---

### Confidence Variance

```
Planning:        Low       [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.7-1.0 range
Knowledge Graph: High      [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 0.5-1.0 range
Causal:          Very High [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] Variable effect sizes
```

**Insight**: Causal tests confidence propagation most rigorously

---

## Expected Stress Tests

| Component | Planning Tests | KG Tests | Causal Tests |
|-----------|---------------|----------|--------------|
| **R-GCN** | 16 edge types | 50+ edge types â­ | 20 edge types |
| **WHY/WHAT** | Goal decomposition â­ | Type abstraction | Cause â†’ observation |
| **Confidence** | Stable values | Partial observability â­ | Effect sizes â­ |
| **Multi-hop** | Prerequisite chains | Entity paths â­ | Causal chains |
| **Interpretability** | Action sequences â­ | Entity relations | Counterfactuals â­ |

â­ = Domain expected to provide strongest test

---

## Evaluation Protocol

### Shared Metrics (All Domains)

```python
def evaluate_domain(model, dataset_name):
    return {
        'reconstruction_error': test_cycle_consistency(model, dataset),
        'task_accuracy': test_prediction_accuracy(model, dataset),
        'confidence_calibration': compute_ECE(model, dataset),
        'multi_hop_accuracy': test_reasoning_chains(model, dataset, k=5),
        'training_convergence': epochs_to_convergence(model, dataset),
        'interpretability': qualitative_assessment(model, dataset)
    }
```

### Domain-Specific Tests

**Planning**:
- âœ“ Goal achievement rate
- âœ“ Invalid sequence detection
- âœ“ Temporal ordering correctness
- âœ“ Capability coverage
- âœ“ Decomposition accuracy

**Knowledge Graph**:
- âœ“ Link prediction (Hits@10, MRR)
- âœ“ Analogical reasoning
- âœ“ Type consistency
- âœ“ Multi-hop reasoning

**Causal**:
- âœ“ Counterfactual accuracy
- âœ“ Confounder detection
- âœ“ Effect size estimation
- âœ“ Causal vs correlational distinction
- âœ“ Intervention prediction

---

## Decision Criteria

As specified in CLAUDE.md NSM-10:

| Criterion | Weight | Planning | Knowledge Graph | Causal |
|-----------|--------|----------|----------------|--------|
| **Task Accuracy** | 40% | Goal achievement | Link prediction | Counterfactual accuracy |
| **Calibration** | 20% | ECE | ECE | ECE |
| **Multi-hop** | 20% | Prerequisite chains | Entity paths | Causal chains |
| **Interpretability** | 20% | Action sequences | Entity relations | Counterfactuals |

**Success Criteria**:
- **Minimum**: â‰¥80% accuracy on 2/3 domains
- **Ideal**: â‰¥80% on all three with same hyperparameters
- **Failure**: Only works on one domain â†’ architecture too specialized

---

## Implementation Status

### âœ… Planning Domain (`dataset-planning`)

**Files Created**:
- `nsm/data/planning_dataset.py` (515 lines)
- `nsm/evaluation/planning_metrics.py` (458 lines)
- `tests/data/test_planning_dataset.py` (493 lines)
- `examples/planning_example.py` (255 lines)

**Test Results**: 25/25 passing âœ“

**Commits**: 5 logical commits

---

### âœ… Knowledge Graph Domain (`dataset-knowledge-graph`)

**Files Created**:
- `nsm/data/knowledge_graph_dataset.py` (682 lines)
- `nsm/evaluation/kg_metrics.py` (449 lines)
- `tests/data/test_kg_dataset.py` (394 lines)
- `examples/knowledge_graph_example.py` (216 lines)
- `KG_IMPLEMENTATION_SUMMARY.md` (245 lines)

**Test Results**: 21/21 passing âœ“ (98% coverage)

**Commits**: 5 logical commits

---

### âœ… Causal Domain (`dataset-causal`)

**Files Created**:
- `nsm/data/causal_dataset.py` (513 lines)
- `nsm/evaluation/causal_metrics.py` (502 lines)
- `tests/data/test_causal_dataset.py` (488 lines)
- `examples/causal_example.py` (353 lines)
- `CAUSAL_DATASET_SUMMARY.md` (not specified)

**Test Results**: 27/27 passing âœ“ (94% coverage)

**Commits**: 5 logical commits

---

## Integration Verification

All three datasets verified compatible with:

| NSM Component | Planning | KG | Causal |
|---------------|----------|-----|--------|
| **NSM-18** (BaseSemanticTripleDataset) | âœ… | âœ… | âœ… |
| **NSM-18** (GraphConstructor) | âœ… | âœ… | âœ… |
| **NSM-18** (TripleVocabulary) | âœ… | âœ… | âœ… |
| **NSM-17** (R-GCN edge types) | âœ… | âœ… | âœ… |
| **NSM-12** (Confidence propagation) | âœ… | âœ… | âœ… |
| **NSM-14** (Training loop ready) | âœ… | âœ… | âœ… |

---

## Next Steps

### 1. Run Evaluation Suite (Week 2-3)

```bash
# In each worktree
cd ../nsm-planning
python -m tests.evaluation_suite --dataset planning --output results/planning.json

cd ../nsm-kg
python -m tests.evaluation_suite --dataset knowledge_graph --output results/kg.json

cd ../nsm-causal
python -m tests.evaluation_suite --dataset causal --output results/causal.json
```

### 2. Comparative Analysis (Week 4)

```bash
# Back in main repo
python compare_results.py results/*.json
```

### 3. Document Findings

Create `results/NSM-10-EVALUATION-REPORT.md` with:
- Performance comparison table
- Domain-specific failure modes
- Architectural recommendations
- Multi-domain training strategy

### 4. Update Parent Issues

- Update NSM-10 with evaluation results
- Update NSM-20 with findings
- Inform design decisions for Phase 2

---

## Recommendations

### Expected Outcomes

**Most Likely**: All three domains work with minor hyperparameter tuning
- NSM architecture is domain-general by design
- BDI-HTN-HRL framework supports all three reasoning types

**If Planning >> Others**:
- Architecture may be over-specialized for procedural reasoning
- Consider adding attention mechanisms for relational reasoning

**If KG >> Others**:
- May need stronger temporal modeling (planning)
- May need causal inference layers (causal)

**If Causal >> Others**:
- Good sign for frontier AI capabilities
- Consider Pearl's do-calculus integration

### Multi-Domain Training

**Recommendation**: Train on all three domains sequentially or jointly

```python
class MultiDomainNSM:
    datasets = {
        'planning': PlanningTripleDataset(),
        'knowledge': KnowledgeGraphTripleDataset(),
        'causal': CausalTripleDataset()
    }

    # Multi-task learning with domain-specific heads
    # OR sequential transfer learning
    # OR ensemble approach
```

---

## Conclusion

âœ… **All three dataset domains successfully implemented**

**Total Effort**:
- ~5,350 lines of code across three branches
- 73 comprehensive tests (25 + 21 + 27)
- Domain-specific evaluation metrics
- Interactive examples for each domain

**Status**: Ready for empirical evaluation and comparison

**Next**: Run evaluation suite, compare results, update NSM-20 with findings

---

## References

- **Linear Issue**: [NSM-10](https://linear.app/imajn/issue/NSM-10)
- **Parent Issue**: [NSM-20](https://linear.app/imajn/issue/NSM-20)
- **Planning Summary**: `../nsm-planning/` (25 tests)
- **KG Summary**: `../nsm-kg/KG_IMPLEMENTATION_SUMMARY.md` (21 tests)
- **Causal Summary**: `../nsm-causal/CAUSAL_DATASET_SUMMARY.md` (27 tests)

---

**Generated**: 2025-10-20
**Claude Code**: claude.ai/code

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
